{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Andrew Martin (eeasm) \\\n",
    "Creation date: 14/12/22\n",
    "\n",
    "#### Known Issues:\n",
    "+ Doesn't actually do anything yet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transferring Summit data\n",
    "\n",
    "This notebook will be to transfer Summit data from the JASMIN ncas gws to another location, subset for a given time frame.\n",
    "\n",
    "For example, if I want the MMCR data for 5 hours surrounding 11/08/2022 15:00 then I can set up the parameters to look within the appropriate timeframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial imports\n",
    "import os\n",
    "import datetime\n",
    "import helper as hh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gws/nopw/j04/ncas_radar_vol1/eeasm/ICESAT/rgt_0749/cycle_10\n"
     ]
    }
   ],
   "source": [
    "# setup where we want the data to be delivered to.\n",
    "# In my case, this is my part of the ncas_radar_gws_vol1\n",
    "endpoint_base = '/gws/nopw/j04/ncas_radar_vol1/eeasm'\n",
    "# steps from within endpoint_base to get to where I want the data to be stored\n",
    "endpoint_path_steps = ['ICESAT','rgt_0749','cycle_10']\n",
    "# the final path that shall be used.\n",
    "endpoint_path = os.path.join(endpoint_base,*endpoint_path_steps)\n",
    "\n",
    "print(endpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATL09_in_endpoint=['processed_ATL09_20210211004659_07491001_005_01.h5']\n",
      "processed_ATL09_20210211004659_07491001_005_01.h5    datetime= 2021-02-11 00:46:59\n"
     ]
    }
   ],
   "source": [
    "# here, we will extract the ICESAT ATL09 file from the given endpoint_path and determine what date it came from. Then we will select the 3 hours either side of this.\n",
    "\n",
    "ATL09_in_endpoint = [f for f in os.listdir(endpoint_path) if 'ATL09' in f]\n",
    "print(f'{ATL09_in_endpoint=}')\n",
    "\n",
    "# for each ATL09 file in the destination, we will extract the date determined by the format of thae filename\n",
    "\n",
    "ATL09_timestamps = []\n",
    "for f in ATL09_in_endpoint:\n",
    "    if 'processed' in f:\n",
    "        # in this instance, icepyx has been used and the filename format is\n",
    "        filename_format = 'processed_ATL09_%Y%m%d%H%M%S'\n",
    "        f = f[:len(filename_format) + 2] # the additional 2 is for the %Y accounting for 4 characters rather than 2\n",
    "    else:\n",
    "        # otherwise, we assume the filename is of the format\n",
    "        filename_format = 'ATL09_%Y%m%d%H%M%S'\n",
    "        f = f[:len(filename_format) + 2]\n",
    "\n",
    "    ATL09_timestamps.append(datetime.datetime.strptime(f,filename_format))\n",
    "\n",
    "for f,d in zip(ATL09_in_endpoint,ATL09_timestamps):\n",
    "    print(f, d.strftime('   datetime= %Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gws/nopw/j04/ncas_radar_vol2/data/ICECAPSarchive/mpl/raw   format='%Y%m%d%H*.mpl.gz'\n",
      "/gws/nopw/j04/ncas_radar_vol2/data/ICECAPSarchive/mmcr/mom   format='%Y%j%H*.nc.zip'\n"
     ]
    }
   ],
   "source": [
    "# setup where the data will be being copied from\n",
    "initial_base = '/gws/nopw/j04/ncas_radar_vol2/data/ICECAPSarchive'\n",
    "\n",
    "mpl_path_steps = ['mpl','raw']\n",
    "mmcr_path_steps = ['mmcr','mom']\n",
    "\n",
    "\n",
    "mpl_path = os.path.join(initial_base,*mpl_path_steps)\n",
    "mmcr_path = os.path.join(initial_base,*mmcr_path_steps)\n",
    "\n",
    "# the file format that the MPL and MMCR files use.\n",
    "# IMPORTANT NOTE: if we want sub-daily partitioning, we MUST include %H into these formats\n",
    "mpl_format = '%Y%m%d%H*.mpl.gz'\n",
    "mmcr_format = '%Y%j%H*.nc.zip' # uses the julian day\n",
    "depth = 'h' # how the helper hh.move_data_files.find_files_in_date_range() will dearch for files\n",
    "width = datetime.timedelta(hours=2) # this can be adjusted\n",
    "\n",
    "extract_paths = [mpl_path, mmcr_path]\n",
    "extract_formats = [mpl_format, mmcr_format]\n",
    "for v,format in zip(extract_paths,extract_formats): print(v, f'  {format=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, False, False]\n",
      "[False, False, False]\n"
     ]
    }
   ],
   "source": [
    "# for each ATL09 file in ATL09_in_endpoint, we will locate the relevant mmcr and mpl files at their locations within the daterange.\n",
    "# We will then check that each hasn't already been moved to the target location.\n",
    "# If not, we will move the file across.\n",
    "\n",
    "for f,d in zip(ATL09_in_endpoint,ATL09_timestamps):\n",
    "    daterange = [d-width, d+width]\n",
    "\n",
    "    # for each type of data file we want to extract: mmcr, mpl, etc\n",
    "    for initial, format in zip(extract_paths,extract_formats):\n",
    "        # extract the filenames for the data files\n",
    "        datafile_names = hh.move_data_files.find_files_in_date_range(initial,daterange,format,depth)\n",
    "        # next, we need to check to see if the files exist at the target location\n",
    "        exists_at_endpoint = [hh.move_data_files._check_file_exists(endpoint_path,dfname) for dfname in datafile_names]\n",
    "        print(exists_at_endpoint)\n",
    "\n",
    "        for dfname,exists in zip(datafile_names,exists_at_endpoint):\n",
    "            if not exists:\n",
    "                success = hh.move_data_files.copy_file(initial,dfname,endpoint_path)\n",
    "                if not success:\n",
    "                    print(f'Trouble copying {dfname=} from {initial=} to {endpoint_path=}')\n",
    "            else:\n",
    "                print(f'{dfname} already exists at {endpoint_path}')\n",
    "\n",
    "print('files copied')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eee37da0082c13e9f615955afd90972b849a8f475b4f1f0a75b8ab1eaa529945"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
